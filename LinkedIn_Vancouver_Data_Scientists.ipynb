{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Vancouver's Data Scientists\"\n",
    "output:\n",
    "  html_notebook: default\n",
    "  html_document: default\n",
    "---\n",
    "\n",
    "I'm going to try and webscrape LinkedIn for information on Vancouver's current data scientist opportunities. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My original introduction into beautiful soup came from: https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/\n",
    "and\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1. is focused on the current job market. \n",
    "\n",
    "I am going to get a list of links for current data scientist (and related) jobs in Vancouver. \n",
    "\n",
    "Then I'll go through each job posting and extract key information. What I think this will be at the beginning is something like: title, company, when it was posted, skills. Different companies phrase these different, but there is usually a list of requirements and then bonus items (nice-to-haves). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the postings I'm looking for. \n",
    "\n",
    "\n",
    "I manually pull up this link using the search for \n",
    "https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=Vancouver%2C%20British%20Columbia%2C%20Canada\n",
    "\n",
    "If you inspect the link you can see the search terms I used (data scientist) and the location (Vancouver, BC).\n",
    "\n",
    "I also can see that they show about 25 posting per 'page' with future pages having the '&start=' tag at the end:\n",
    "https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=Vancouver%2C%20British%20Columbia%2C%20Canada&start=175\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0\n",
    "from selenium.webdriver.support import expected_conditions as EC # available since 2.26.0\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#jobs_pages\n",
    "jobs_page = \"https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=Vancouver%2C%20British%20Columbia%2C%20Canada&sortBy=DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_urls = []\n",
    "driver = Chrome()\n",
    "driver.get(jobs_page)\n",
    "time.sleep(3)\n",
    "more_jobs_path = '/html/body/main/section[1]/button'\n",
    "for i in range(1,6):\n",
    "    time.sleep(3)\n",
    "    print(i)\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, more_jobs_path)))\n",
    "        jobs_button = driver.find_elements_by_xpath(more_jobs_path)[0]\n",
    "        jobs_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"No more buttons to push\")\n",
    "mydivs = driver.find_elements_by_class_name(\"result-card__full-card-link\")\n",
    "for div in mydivs:\n",
    "    job_url = div.get_attribute(\"href\")\n",
    "    job_urls.append(job_url)\n",
    "    #print(job_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Often the information is stored in bulleted lists (\\<li>), but in this first page we can see that there is a bulleted list for 'Essential skills' and 'Assets'. \n",
    "\n",
    "While it might be messier, I might have to grab all the description information (\\<section class=\"description\">) for each page and sift through later. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#company_result = soup.find_all(\"a\", class_=\"topcard__org-name-link\")\n",
    "\n",
    "#company = str(company_result)\n",
    "#company = company.split(\">\")[1]\n",
    "#company = company.split(\"<\")[0]\n",
    "\n",
    "#I realized I should write a function for this instead of copy and pasting. \n",
    "\n",
    "def return_string(soup_result):\n",
    "    text = str(soup_result)\n",
    "    text = text.split(\">\")[1]\n",
    "    text = text.split(\"<\")[0]\n",
    "    return(text)\n",
    "\n",
    "#return_string(company_result)\n",
    "#return_string(title_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clean description\n",
    "#After looking at it, there's going to be so much variability between them that this'll be a challenge. \n",
    "import re\n",
    "def clean_description(description_result):\n",
    "    description_text = str(description_result)\n",
    "    description_text = re.sub('<[^<]+?>',\" \", description_text)\n",
    "    description_text = re.sub('  ',\" \", description_text)\n",
    "    return(description_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_output = []\n",
    "for job_url in job_urls:\n",
    "    #print(job_url) #You can uncomment this if you want to make sure it is progressing through the web pages\n",
    "    \n",
    "    time.sleep(random.randint(10,100)/100) #I generally put in a random time-delay to not overload servers and not get banned\n",
    "    try: \n",
    "        req = Request(job_url, headers= {'User-Agent': 'Mozilla/5.0'})\n",
    "        page = urlopen(req).read()\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        \n",
    "        #Get raw results\n",
    "        title_result = soup.find_all(\"h1\")\n",
    "        company_result = soup.find_all(\"a\", class_=\"topcard__org-name-link\")\n",
    "        description_result = soup.find_all(\"section\", class_=\"description\")\n",
    "        \n",
    "        #Clean results\n",
    "        title = return_string(title_result)\n",
    "        company = return_string(company_result)\n",
    "        description = clean_description(description_result)\n",
    "        \n",
    "        #Compile results into dictionary\n",
    "        job_info = {\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'url' : job_url,\n",
    "            'description' : description,\n",
    "            'description_raw': str(description_result)\n",
    "        }\n",
    "        del description_result\n",
    "        del title_result\n",
    "        del company_result\n",
    "        #Append dictionary to list\n",
    "        job_output.append(job_info)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_output #Very long text! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our data looks like \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#job_output\n",
    "\n",
    "jobs_df = pd.DataFrame(job_output)\n",
    "\n",
    "jobs_df = jobs_df.sort_values(\"company\")\n",
    "#jobs_df[\"description\"] = str(jobs_df[\"description\"])\n",
    "jobs_df[\"description\"] = jobs_df[\"description\"].astype(str) \n",
    "#del jobs_df[\"url\"]\n",
    "jobs_df = jobs_df.drop_duplicates()\n",
    "\n",
    "#jobs_df[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write results:\n",
    "jobs_df.to_csv(\"linkedin_data.csv\", index=False)\n",
    "\n",
    "#I write the results to a file to a) store the data, and b) be able to pick up here for working on this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we've got our dataset of jobs with the rough description text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "jobs_df = pd.read_csv(\"linkedin_data.csv\")\n",
    "#jobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most important parts I'd like to get out are the frequency of skills in the 'Requirements' and 'Assets' lists in each job posting. Unfortunately, they all use different language for these. I'm going to try and write a function that returns all the list entries after an initial word and stores the entries and the starting word for a jobid. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_starters = [\"Required Skills And Experience\", \"Must have\", \"Qualifications\"]\n",
    "\n",
    "requirements = [\"Qualifications\", \"Requirements\", \"Required Skills And Experience\", \"Background\", \"background includes\",  \"Skills\", \"Needs\", \"Basic Qualifications\", \"Must Have\"]\n",
    "responsibilities = [\"Responsibilities\", \"Your Role Is To\", \"Whatâ€™s the job?\"]\n",
    "assets = [\"Assets\", \"Additional skills\", \"Charm Us With\", \"Preferred Qualifications\", \"Additional\", \"Bonus\", \"we'd love to see you have\"]\n",
    "\n",
    "\n",
    "false_list = [False] * len(requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#example_text = jobs_df.iloc[1,3]\n",
    "#example_text\n",
    "\n",
    "def remove_escape_chars(text):\n",
    "        text = text.replace(\"-\", \"\")\n",
    "        text = text.replace(\"[\", \"\")\n",
    "        text = text.replace(\"{\", \"\")\n",
    "        text = text.replace(\"(\", \"\")\n",
    "        return(text)\n",
    "    \n",
    "    \n",
    "def return_list(text, list_starters):\n",
    "    jobs_quals = []\n",
    "    \n",
    "    text = text.lower()\n",
    "    list_starters = [x.lower() for x in list_starters]\n",
    "    split_list = text.split(\"<li>\")\n",
    "    text = remove_escape_chars(text)\n",
    "    text = remove_escape_chars(text)#Need to run twice because of \"--\"\n",
    "    false_list = [False] * len(list_starters)\n",
    "    for i in range(0,len(split_list)):\n",
    "        text = split_list[i]\n",
    "        if [x in text for x in list_starters] != false_list:\n",
    "            y = [x in text for x in list_starters]\n",
    "            #print(y)\n",
    "            list_starter = np.array(list_starters)[np.array(y)]\n",
    "            list_starter = str(list_starter[0])\n",
    "            \n",
    "            \n",
    "            job_quals = []\n",
    "            for n in range(i+1, len(split_list)):           \n",
    "                if \"</li>\" in split_list[n]:\n",
    "                    qual = split_list[n]\n",
    "                    qual = qual.split(\"</li>\")[0]\n",
    "                    job_quals.append(qual)\n",
    "                    #print(\"List item\")\n",
    "            job_dict = {list_starter : job_quals}\n",
    "            jobs_quals.append(job_dict)\n",
    "    return(jobs_quals)\n",
    "    \n",
    "\n",
    "#return_list(example_text, list_starters=requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs_df[\"Requirements\"] = jobs_df[\"description_raw\"].apply(return_list, list_starters=requirements)\n",
    "jobs_df[\"Responsibilities\"] = jobs_df[\"description_raw\"].apply(return_list, list_starters=responsibilities)\n",
    "jobs_df[\"Assets\"] = jobs_df[\"description_raw\"].apply(return_list, list_starters=assets)\n",
    "\n",
    "jobs_df[\"Requirements\"].astype(str).str.find(\"qualifications\")\n",
    "\n",
    "#jobs_df\n",
    "for n in range(1, len(jobs_df)):\n",
    "    if len(jobs_df.loc[n, \"Requirements\"]) > 0:        \n",
    "        if len(jobs_df.loc[n, \"Assets\"]) > 0:\n",
    "            for i in range(0, len(jobs_df.loc[n, \"Requirements\"])):\n",
    "                req_dict = jobs_df.loc[n, \"Requirements\"][i].values()\n",
    "                for x in range(0, len(jobs_df.loc[n, \"Assets\"])):\n",
    "                    asset_dict = jobs_df.loc[n, \"Assets\"][x].values()\n",
    "                    if asset_dict==req_dict:\n",
    "                        print(\"1\")\n",
    "            \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information from these:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_df[\"Requirements\"] = jobs_df[\"Requirements\"].astype(str) \n",
    "jobs_df[\"Requirements\"] = jobs_df[\"Requirements\"].str.lower()\n",
    "jobs_df[\"Assets\"] = jobs_df[\"Assets\"].astype(str) \n",
    "jobs_df[\"Assets\"] = jobs_df[\"Assets\"].str.lower()\n",
    "jobs_df[\"Responsibilities\"] = jobs_df[\"Responsibilities\"].astype(str) \n",
    "jobs_df[\"Responsibilities\"] = jobs_df[\"Responsibilities\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_attribute(df, colname, search_column, search_string=None):\n",
    "    z = pd.Series(0, index=jobs_df.index)\n",
    "    if search_string==None:\n",
    "        search_string = colname\n",
    "    if type(search_string)==str:\n",
    "        z = jobs_df[search_column].str.find(search_string)\n",
    "    if type(search_string)==list:\n",
    "        for x in search_string:    \n",
    "            y = jobs_df[search_column].str.find(x)\n",
    "            z = z + y\n",
    "    df[colname] = z  \n",
    "    del z \n",
    "    df[colname] = df[colname].apply(lambda x: 1 if x >0 else 0)\n",
    "    return(df)\n",
    "\n",
    "jobs_df[\"All\"] = jobs_df[\"Requirements\"].astype(str)  + jobs_df[\"Assets\"].astype(str) \n",
    "\n",
    "jobs_df[\"All\"] = jobs_df[\"All\"].str.lower()\n",
    "\n",
    "\n",
    "simple_searches = [\"bigquery\", \"python\", \"sql\", \"jira\", \"tableau\", \"docker\", \"scala\", \"java\", \"spark\", \"hadoop\",\n",
    "                  \"statistics\", \"nlp\", \"cnn\", \"rnn\", \"programming\"]\n",
    "cols = [\"Requirements\", \"Assets\"]\n",
    "\n",
    "for col in cols:\n",
    "    for search in simple_searches:\n",
    "        job_attribute(df=jobs_df, colname=search+\"_\"+col, search_string=search, search_column=col)\n",
    "\n",
    "\n",
    "complex_searches = {\n",
    "    \"R\": [\" r,\", \" R or\"],\n",
    "    \"bachelor\" : [\"bachelor\", \"bs/ms\"],\n",
    "    \"master\" : [\"master\", \"bs/ms\", \" ms \", \"graduate degree\"],\n",
    "    \"phd\" : [\"phd\",\"ph\\.d\" \"graduate degree\"],\n",
    "    \"C\":  [\"c\\+\", \"c\\+\\+\", \"c#\"],\n",
    "    \"ML\": [\" ml\", \"machine learning\"],\n",
    "    \"CS\":  [\"computer science\", \" cs\"],\n",
    "    \"SAS\": [\" sas \", \" sas,\", \" sas\\.\"],\n",
    "    \"AI\": [\" ai \", \" ai,\", \"ai \"]\n",
    "}\n",
    "\n",
    "for col in cols:\n",
    "    for search in complex_searches:\n",
    "        job_attribute(df=jobs_df, colname=search+\"_\"+col, search_string=complex_searches[search], search_column=col)\n",
    "\n",
    "\n",
    "\n",
    "jobs_df[\"master_Assets\"].loc[jobs_df[\"master_Requirements\"]==1] = 0\n",
    "jobs_df[\"phd_Assets\"].loc[jobs_df[\"phd_Requirements\"]==1] = 0\n",
    "\n",
    "\n",
    "#jobs_df[\"graduate_edu_all\"] = 0 \n",
    "#jobs_df[\"graduate_edu_all\"].loc[jobs_df[\"master_all\"]==1] = 1\n",
    "#jobs_df[\"graduate_edu_all\"].loc[jobs_df[\"phd_all\"]==1] = 1\n",
    "\n",
    "jobs_df[\"graduateEducation_Requirements\"] = 0 \n",
    "jobs_df[\"graduateEducation_Requirements\"].loc[jobs_df[\"master_Requirements\"]==1] = 1\n",
    "jobs_df[\"graduateEducation_Requirements\"].loc[jobs_df[\"phd_Requirements\"]==1] = 1\n",
    "\n",
    "jobs_df[\"university_Requirements\"] = 0 \n",
    "jobs_df[\"university_Requirements\"].loc[jobs_df[\"master_Requirements\"]==1] = 1\n",
    "jobs_df[\"university_Requirements\"].loc[jobs_df[\"phd_Requirements\"]==1] = 1\n",
    "jobs_df[\"university_Requirements\"].loc[jobs_df[\"bachelor_Requirements\"]==1] = 1\n",
    "\n",
    "#jobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_summary = jobs_df.iloc[:, 10:len(jobs_df.columns)]\n",
    "skills_summary = skills_summary.melt()\n",
    "skills_summary = skills_summary.groupby('variable', as_index=False).sum()\n",
    "\n",
    "skills_summary_df = pd.DataFrame(skills_summary)\n",
    "\n",
    "skills_summary_df[\"variable\"] = skills_summary_df[\"variable\"].astype(str)\n",
    "new_col = skills_summary_df[\"variable\"].str.split(pat='_', expand=True)\n",
    "skills_summary_df[\"attribute\"] = new_col[0]\n",
    "skills_summary_df[\"type\"] = new_col[1]\n",
    "\n",
    "#skills_summary_df.columns = [\"variable\", \"value\"]\n",
    "\n",
    "\n",
    "skills_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Old vertsion of sort_df function. Doesn't need to be this complicated and can't handle non-unique categories. \n",
    "def sort_df(df, var_col=\"variable\", val_col=\"value\", ascending=False):\n",
    "    var_ordered = df[var_col][df[val_col].sort_values(ascending=ascending).index.tolist()]    \n",
    "    df[var_col] = pd.Categorical(df[var_col], categories=list(reversed(list(var_ordered))), ordered=True)\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "import seaborn as sns\n",
    "#Set plot aesthetics:\n",
    "p9.theme_set(p9.theme_classic())\n",
    "\n",
    "\n",
    "def sort_df(df, var_col, val_col=\"value\", ascending=False):\n",
    "    if len(set(var_col))<len(df):\n",
    "        df_temp = df.groupby(var_col, as_index=False).sum()\n",
    "        var_ordered = df_temp[var_col][df_temp[val_col].sort_values(ascending=ascending).index.tolist()] \n",
    "    else:\n",
    "        var_ordered = df[var_col][df[val_col].sort_values(ascending=ascending).index.tolist()]    \n",
    "    df[var_col] = pd.Categorical(df[var_col], categories=list(reversed(list(var_ordered))), ordered=True)\n",
    "    return(df)\n",
    "skills_summary_df = sort_df(skills_summary_df, var_col=\"attribute\")\n",
    "skills_summary_df[\"type\"] = pd.Categorical(skills_summary_df[\"type\"])\n",
    "\n",
    "skills_summary_df[\"type\"] = skills_summary_df[\"type\"].cat.reorder_categories([\"Requirements\", \"Assets\"])\n",
    "\n",
    "(p9.ggplot(skills_summary_df, p9.aes('attribute', 'value', fill='variable')) + \n",
    " p9.geom_col() +\n",
    " p9.coord_flip() +\n",
    " p9.scale_fill_discrete(guide=False)\n",
    ")\n",
    "#skills_summary_df[\"type\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This graph above is real ugly, but it does show that our code is in general workiing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Languages\n",
    "languages = [\"R\", \"sql\", \"python\", \"java\", \"scala\", \"C\", \"sas\"]\n",
    "\n",
    "lang_clean = {\"sql\": \"SQL\",\n",
    "             \"python\": \"Python\",\n",
    "             \"R\": \"R\",\n",
    "             \"java\": \"Java\",\n",
    "             \"scala\": \"Scala\",\n",
    "             \"C\": \"C\",\n",
    "             \"sas\": \"SAS\"}\n",
    "\n",
    "skills_summary_lang = skills_summary_df[skills_summary_df.attribute.isin(languages)]\n",
    "skills_summary_lang = skills_summary_lang.replace(to_replace=lang_clean)\n",
    "skills_summary_lang = sort_df(skills_summary_lang, var_col=\"attribute\")\n",
    "\n",
    "\n",
    "(p9.ggplot(skills_summary_lang, p9.aes('attribute', 'value', fill='type', show_legend=False)) + \n",
    " p9.geom_col() + \n",
    " p9.coord_flip() + \n",
    " p9.scale_y_continuous(expand=[0,0]) + \n",
    " p9.labs(y=\"Frequency\", x=\"Language\", fill=\"\") +\n",
    " p9.scale_fill_brewer(palette=\"Blues\") +\n",
    " p9.facet_wrap('~type')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "programs = [\"tableau\", \"docker\", \"bigquery\", \"jira\", \"spark\", \"hadoop\"]\n",
    "\n",
    "prog_clean = {\"tableau\": \"Tableau\",\n",
    "             \"docker\": \"Docker\",\n",
    "             \"bigquery\": \"Google BigQuery\",\n",
    "             \"jira\" : \"Jira\",\n",
    "             \"spark\": \"Spark\",\n",
    "             \"hadoop\": \"Hadoop\"}\n",
    "\n",
    "skills_summary_prog = skills_summary_df[skills_summary_df.attribute.isin(programs)]\n",
    "skills_summary_prog = skills_summary_prog.replace(to_replace=prog_clean)\n",
    "\n",
    "skills_summary_prog = sort_df(skills_summary_prog, var_col=\"attribute\")\n",
    "\n",
    "(p9.ggplot(skills_summary_prog, p9.aes('attribute', 'value', fill='type', show_legend=False)) + \n",
    " p9.geom_col() + \n",
    " p9.coord_flip() + \n",
    " p9.scale_y_continuous(expand=[0,0]) + \n",
    " p9.labs(y=\"Frequency\", x=\"Program\") +\n",
    " p9.scale_fill_brewer(palette= \"Greens\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Education basic\n",
    "\n",
    "education = [\"bachelor\", \"master\", \"phd\"]\n",
    "\n",
    "education_clean = {\"bachelor\": \"Bachelor's\",\n",
    "             \"master\": \"Master's\",\n",
    "             \"phd\": \"PhD\"}\n",
    "\n",
    "skills_summary_edu = skills_summary_df[skills_summary_df.attribute.isin(education)]\n",
    "skills_summary_edu = skills_summary_edu.replace(to_replace=education_clean)\n",
    "\n",
    "(p9.ggplot(skills_summary_edu, p9.aes('attribute', 'value', fill='type', show_legend=False)) + \n",
    " p9.geom_col() + \n",
    " p9.coord_flip() + \n",
    " p9.scale_y_continuous(expand=[0,0]) + \n",
    " p9.labs(y=\"Frequency\", x=\"Education\", fill=\"\") +\n",
    " p9.scale_fill_brewer(type=\"qual\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knowledge = [\"CS\", \"AI\", \"stats\", \"programming\", \"ML\"]\n",
    "\n",
    "knowledge_clean = {\"AI\": \"AI\",\n",
    "             \"CS\": \"Computer Science\",\n",
    "             \"stats\": \"Statistics\",\n",
    "                \"programming\": \"Programming\"}\n",
    "\n",
    "skills_summary_know = skills_summary_df[skills_summary_df.attribute.isin(knowledge)]\n",
    "skills_summary_know = skills_summary_know.replace(to_replace=knowledge_clean)\n",
    "skills_summary_know = sort_df(skills_summary_know, var_col=\"attribute\")\n",
    "\n",
    "(p9.ggplot(skills_summary_know, p9.aes('attribute', 'value', fill='type', show_legend=False)) + \n",
    " p9.geom_col() + \n",
    " p9.coord_flip() + \n",
    " p9.scale_y_continuous(expand=[0,0]) + \n",
    " p9.labs(y=\"Frequency\", x=\"Education\", fill=\"\") +\n",
    " p9.scale_fill_brewer(palette= \"YlOrRd\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niche = [\"cnn\", \"rnn\",  \"nlp\"]\n",
    "\n",
    "niche_clean = {\n",
    "             \"cnn\": \"CNN\",\n",
    "             \"rnn\": \"RNN\",\n",
    "                \"nlp\": \"NLP\"}\n",
    "def sort_df(df, var_col, val_col=\"value\", ascending=False):\n",
    "    if len(set(df[var_col]))<len(df):\n",
    "        df_temp = df.groupby(var_col, as_index=False).sum()\n",
    "        var_ordered = df_temp[var_col][df_temp[val_col].sort_values(ascending=ascending).index.tolist()] \n",
    "    else:\n",
    "        var_ordered = df[var_col][df[val_col].sort_values(ascending=ascending).index.tolist()]    \n",
    "    df[var_col] = pd.Categorical(df[var_col], categories=list(reversed(list(var_ordered))), ordered=True)\n",
    "    return(df)\n",
    "\n",
    "skills_summary_niche = skills_summary_df[skills_summary_df.attribute.isin(niche)]\n",
    "skills_summary_niche = skills_summary_niche.replace(to_replace=niche_clean)\n",
    "skills_summary_niche = sort_df(skills_summary_niche, var_col=\"attribute\")\n",
    "\n",
    "(p9.ggplot(skills_summary_niche, p9.aes('attribute', 'value', fill='type', show_legend=False)) + \n",
    " p9.geom_col() + \n",
    " p9.coord_flip() + \n",
    " p9.scale_y_continuous(expand=[0,0]) + \n",
    " p9.labs(y=\"Frequency\", x=\"Niche knowledge areas\", fill=\"\") +\n",
    " p9.scale_fill_brewer(palette=\"Purples\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistakes I made:\n",
    "- I originally didn't sort the job postings by some feature (i.e., by date and so 'relevance' could change over time)\n",
    "- My original code was only giving me job postings from one page even though I was looping over multiple. When I test the links individually, they worked. I fixed this by switching to selenium and not trying to go to the new pages as separate requests but by loading more jobs on their infinite scroll system.\n",
    "- I had to go back to redefine some functions to make sure they were returning the right information, and I wanted to know more about where the information (e.g., skill, language or topic area) was coming from in the job posting. \n",
    "- Probably lots more. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
